{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "245da44c",
   "metadata": {},
   "source": [
    "from Collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "565f7af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "def normalized_laplacian(adj_matrix):\n",
    "    R = np.sum(adj_matrix, axis=1)\n",
    "    adj_matrix = adj_matrix + np.eye(adj_matrix.shape[0])\n",
    "    R_sqrt = 1/np.sqrt(R)\n",
    "    D_sqrt = np.diag(R_sqrt) + np.eye(adj_matrix.shape[0])\n",
    "    return np.matmul(np.matmul(D_sqrt, adj_matrix), D_sqrt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7663fecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/SC_Full_S13_data.pth\n",
      "dataset/labels_Full_S13_1.pth\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "# graph\n",
    "batch = 0\n",
    "data_dir = glob.glob('dataset/SC_Full_S*_data.pth')\n",
    "all_label = []\n",
    "all_data = []\n",
    "all_data_graph = []\n",
    "batch =0\n",
    "flag = 0\n",
    "data_dir = sorted(data_dir)\n",
    "for index in data_dir:\n",
    "    data = torch.load(index)\n",
    "    data = torch.stack(data)\n",
    "    data += 1e-3*torch.eye(data.shape[-1])\n",
    "    data =  data\n",
    "    if flag == 0:\n",
    "        all_data_graph = data\n",
    "        flag = 1\n",
    "    else:\n",
    "        all_data_graph = torch.cat((all_data_graph, data),dim =0)\n",
    "    batch+=1\n",
    "    print(index)\n",
    "\n",
    "# data\n",
    "batch = 0\n",
    "data_dir = glob.glob('dataset/FC_Full_S*_data.pth')\n",
    "all_label = []\n",
    "all_data = []\n",
    "batch =0\n",
    "flag = 0\n",
    "data_dir = sorted(data_dir)\n",
    "for index in data_dir:\n",
    "    data = torch.load(index)\n",
    "    data = torch.stack(data)\n",
    "    data += 1e-3*torch.eye(data.shape[-1])\n",
    "    if flag == 0:\n",
    "        all_data = data\n",
    "        flag = 1\n",
    "    else:\n",
    "        all_data = torch.cat((all_data, data),dim =0)\n",
    "    batch += 1\n",
    "\n",
    "batch = 0\n",
    "all_label_dir = glob.glob('dataset/labels_Full_S*_1.pth')\n",
    "all_label_dir = sorted(all_label_dir)\n",
    "flag = 0\n",
    "for index in all_label_dir:\n",
    "    data = torch.load(index)\n",
    "    data = torch.stack(data)\n",
    "    # data += 1e-3*torch.eye(data.shape[-1])\n",
    "    if flag == 0:\n",
    "        all_label = data\n",
    "        flag = 1\n",
    "    else:\n",
    "        all_label = torch.cat((all_label,data),dim =0)\n",
    "    batch += 1\n",
    "    print(index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdbbc18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bisect\n",
    "import math\n",
    "import torch\n",
    "class LRScheduler:\n",
    "    def __init__(self):\n",
    "        self.lr_config = 0\n",
    "        self.training_config =0\n",
    "        self.lr = 0\n",
    "\n",
    "    def update(self, optimizer: torch.optim.Optimizer, step: int):\n",
    "        lr_mode = 'cos'\n",
    "        base_lr = 1.0e-4\n",
    "        target_lr = 1.0e-6\n",
    "\n",
    "        warm_up_from = 0.0\n",
    "        warm_up_steps = 0\n",
    "        # total_steps = self.training_config.total_steps\n",
    "        total_steps = 11200\n",
    "        milestones = [0.3, 0.6, 0.9]\n",
    "        lr_decay = 0.98\n",
    "        decay_factor = 0.1\n",
    "        poly_power = 2.0\n",
    "        assert 0 <= step <= total_steps\n",
    "        if step < warm_up_steps:\n",
    "            current_ratio = step / warm_up_steps\n",
    "            self.lr = warm_up_from + (base_lr - warm_up_from) * current_ratio\n",
    "        else:\n",
    "            current_ratio = (step - warm_up_steps) / \\\n",
    "                (total_steps - warm_up_steps)\n",
    "            if lr_mode == 'step':\n",
    "                count = bisect.bisect_left(milestones, current_ratio)\n",
    "                self.lr = base_lr * pow(decay_factor, count)\n",
    "            elif lr_mode == 'poly':\n",
    "                poly = pow(1 - current_ratio, poly_power)\n",
    "                self.lr = target_lr + (base_lr - target_lr) * poly\n",
    "            elif lr_mode == 'cos':\n",
    "                cosine = math.cos(math.pi * current_ratio)\n",
    "                self.lr = target_lr + (base_lr - target_lr) * (1 + cosine) / 2\n",
    "            elif lr_mode == 'linear':\n",
    "                self.lr = target_lr + \\\n",
    "                    (base_lr - target_lr) * (1 - current_ratio)\n",
    "            elif lr_mode == 'decay':\n",
    "                epoch = step // self.training_config.steps_per_epoch\n",
    "                self.lr = base_lr * lr_decay ** epoch\n",
    "\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = self.lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7851360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eopch: 0\n",
      "Precision: 0.444444 recall: 0.666667  | accracy:0.66667  | f1_score:0.53333  | auc:0.44444\n",
      "Eopch: 1\n",
      "Precision: 0.444444 recall: 0.666667  | accracy:0.66667  | f1_score:0.53333  | auc:0.22222\n",
      "Eopch: 2\n",
      "Precision: 0.444444 recall: 0.666667  | accracy:0.66667  | f1_score:0.53333  | auc:0.22222\n",
      "Eopch: 3\n",
      "Precision: 0.444444 recall: 0.666667  | accracy:0.66667  | f1_score:0.53333  | auc:0.27778\n",
      "Eopch: 4\n",
      "Precision: 0.444444 recall: 0.666667  | accracy:0.66667  | f1_score:0.53333  | auc:0.33333\n",
      "Eopch: 5\n",
      "Precision: 0.444444 recall: 0.666667  | accracy:0.66667  | f1_score:0.53333  | auc:0.33333\n",
      "Eopch: 6\n",
      "Precision: 0.444444 recall: 0.666667  | accracy:0.66667  | f1_score:0.53333  | auc:0.44444\n",
      "Eopch: 7\n",
      "Precision: 0.444444 recall: 0.666667  | accracy:0.66667  | f1_score:0.53333  | auc:0.44444\n",
      "Eopch: 8\n",
      "Precision: 0.444444 recall: 0.666667  | accracy:0.66667  | f1_score:0.53333  | auc:0.44444\n",
      "Eopch: 9\n",
      "Precision: 0.444444 recall: 0.666667  | accracy:0.66667  | f1_score:0.53333  | auc:0.44444\n",
      "Eopch: 10\n",
      "Precision: 0.444444 recall: 0.666667  | accracy:0.66667  | f1_score:0.53333  | auc:0.44444\n",
      "Eopch: 11\n",
      "Precision: 0.444444 recall: 0.666667  | accracy:0.66667  | f1_score:0.53333  | auc:0.50000\n",
      "Eopch: 12\n",
      "Precision: 0.444444 recall: 0.666667  | accracy:0.66667  | f1_score:0.53333  | auc:0.50000\n",
      "Eopch: 13\n",
      "Precision: 0.416667 recall: 0.555556  | accracy:0.55556  | f1_score:0.47619  | auc:0.50000\n",
      "Eopch: 14\n",
      "Precision: 0.416667 recall: 0.555556  | accracy:0.55556  | f1_score:0.47619  | auc:0.50000\n",
      "Eopch: 15\n",
      "Precision: 0.642857 recall: 0.666667  | accracy:0.66667  | f1_score:0.64615  | auc:0.55556\n",
      "Eopch: 16\n",
      "Precision: 0.633333 recall: 0.555556  | accracy:0.55556  | f1_score:0.56667  | auc:0.55556\n",
      "Eopch: 17\n",
      "Precision: 0.633333 recall: 0.555556  | accracy:0.55556  | f1_score:0.56667  | auc:0.66667\n",
      "Eopch: 18\n",
      "Precision: 0.633333 recall: 0.555556  | accracy:0.55556  | f1_score:0.56667  | auc:0.72222\n",
      "Eopch: 19\n",
      "Precision: 0.633333 recall: 0.555556  | accracy:0.55556  | f1_score:0.56667  | auc:0.72222\n",
      "Eopch: 20\n",
      "Precision: 0.633333 recall: 0.555556  | accracy:0.55556  | f1_score:0.56667  | auc:0.72222\n",
      "Eopch: 21\n",
      "Precision: 0.700000 recall: 0.666667  | accracy:0.66667  | f1_score:0.67532  | auc:0.72222\n",
      "Eopch: 22\n",
      "Precision: 0.777778 recall: 0.777778  | accracy:0.77778  | f1_score:0.77778  | auc:0.72222\n",
      "Eopch: 23\n",
      "Precision: 0.777778 recall: 0.777778  | accracy:0.77778  | f1_score:0.77778  | auc:0.72222\n",
      "Eopch: 24\n",
      "Precision: 0.777778 recall: 0.777778  | accracy:0.77778  | f1_score:0.77778  | auc:0.77778\n",
      "Eopch: 25\n",
      "Precision: 0.777778 recall: 0.777778  | accracy:0.77778  | f1_score:0.77778  | auc:0.77778\n",
      "Eopch: 26\n",
      "Precision: 0.777778 recall: 0.777778  | accracy:0.77778  | f1_score:0.77778  | auc:0.77778\n",
      "Eopch: 27\n",
      "Precision: 0.777778 recall: 0.777778  | accracy:0.77778  | f1_score:0.77778  | auc:0.77778\n",
      "Eopch: 28\n",
      "Precision: 0.777778 recall: 0.777778  | accracy:0.77778  | f1_score:0.77778  | auc:0.77778\n",
      "Eopch: 29\n",
      "Precision: 0.777778 recall: 0.777778  | accracy:0.77778  | f1_score:0.77778  | auc:0.77778\n",
      "Eopch: 30\n",
      "Precision: 0.777778 recall: 0.777778  | accracy:0.77778  | f1_score:0.77778  | auc:0.77778\n",
      "Eopch: 31\n",
      "Precision: 0.777778 recall: 0.777778  | accracy:0.77778  | f1_score:0.77778  | auc:0.77778\n",
      "Eopch: 32\n",
      "Precision: 0.777778 recall: 0.777778  | accracy:0.77778  | f1_score:0.77778  | auc:0.77778\n",
      "Eopch: 33\n",
      "Precision: 0.777778 recall: 0.777778  | accracy:0.77778  | f1_score:0.77778  | auc:0.77778\n",
      "Eopch: 34\n",
      "Precision: 0.777778 recall: 0.777778  | accracy:0.77778  | f1_score:0.77778  | auc:0.77778\n",
      "Eopch: 35\n",
      "Precision: 0.777778 recall: 0.777778  | accracy:0.77778  | f1_score:0.77778  | auc:0.77778\n",
      "Eopch: 36\n",
      "Precision: 0.777778 recall: 0.777778  | accracy:0.77778  | f1_score:0.77778  | auc:0.88889\n",
      "Eopch: 37\n",
      "Precision: 0.777778 recall: 0.777778  | accracy:0.77778  | f1_score:0.77778  | auc:0.88889\n",
      "Eopch: 38\n",
      "Precision: 0.777778 recall: 0.777778  | accracy:0.77778  | f1_score:0.77778  | auc:0.83333\n",
      "Eopch: 39\n",
      "Precision: 0.777778 recall: 0.777778  | accracy:0.77778  | f1_score:0.77778  | auc:0.83333\n",
      "Eopch: 40\n",
      "Precision: 0.777778 recall: 0.777778  | accracy:0.77778  | f1_score:0.77778  | auc:0.83333\n",
      "Eopch: 41\n",
      "Precision: 0.777778 recall: 0.777778  | accracy:0.77778  | f1_score:0.77778  | auc:0.83333\n",
      "Eopch: 42\n",
      "Precision: 0.777778 recall: 0.777778  | accracy:0.77778  | f1_score:0.77778  | auc:0.83333\n",
      "Eopch: 43\n",
      "Precision: 0.777778 recall: 0.777778  | accracy:0.77778  | f1_score:0.77778  | auc:0.83333\n",
      "Eopch: 44\n",
      "Precision: 0.777778 recall: 0.777778  | accracy:0.77778  | f1_score:0.77778  | auc:0.83333\n",
      "Eopch: 45\n",
      "Precision: 0.777778 recall: 0.777778  | accracy:0.77778  | f1_score:0.77778  | auc:0.83333\n",
      "Eopch: 46\n",
      "Precision: 0.700000 recall: 0.666667  | accracy:0.66667  | f1_score:0.67532  | auc:0.83333\n",
      "Eopch: 47\n",
      "Precision: 0.633333 recall: 0.555556  | accracy:0.55556  | f1_score:0.56667  | auc:0.83333\n",
      "Eopch: 48\n",
      "Precision: 0.633333 recall: 0.555556  | accracy:0.55556  | f1_score:0.56667  | auc:0.83333\n",
      "Eopch: 49\n",
      "Precision: 0.633333 recall: 0.555556  | accracy:0.55556  | f1_score:0.56667  | auc:0.77778\n",
      "Eopch: 0\n",
      "Precision: 0.444444 recall: 0.666667  | accracy:0.66667  | f1_score:0.53333  | auc:0.38889\n",
      "Eopch: 1\n",
      "Precision: 0.833333 recall: 0.777778  | accracy:0.77778  | f1_score:0.73810  | auc:0.44444\n",
      "Eopch: 2\n",
      "Precision: 0.277778 recall: 0.222222  | accracy:0.22222  | f1_score:0.22222  | auc:0.44444\n",
      "Eopch: 3\n",
      "Precision: 0.833333 recall: 0.777778  | accracy:0.77778  | f1_score:0.73810  | auc:0.44444\n",
      "Eopch: 4\n",
      "Precision: 0.833333 recall: 0.777778  | accracy:0.77778  | f1_score:0.73810  | auc:0.38889\n",
      "Eopch: 5\n",
      "Precision: 0.833333 recall: 0.777778  | accracy:0.77778  | f1_score:0.73810  | auc:0.44444\n",
      "Eopch: 6\n",
      "Precision: 0.642857 recall: 0.666667  | accracy:0.66667  | f1_score:0.64615  | auc:0.44444\n",
      "Eopch: 7\n",
      "Precision: 0.483333 recall: 0.444444  | accracy:0.44444  | f1_score:0.45887  | auc:0.44444\n",
      "Eopch: 8\n",
      "Precision: 0.483333 recall: 0.444444  | accracy:0.44444  | f1_score:0.45887  | auc:0.38889\n",
      "Eopch: 9\n",
      "Precision: 0.483333 recall: 0.444444  | accracy:0.44444  | f1_score:0.45887  | auc:0.38889\n",
      "Eopch: 10\n",
      "Precision: 0.483333 recall: 0.444444  | accracy:0.44444  | f1_score:0.45887  | auc:0.33333\n",
      "Eopch: 11\n",
      "Precision: 0.483333 recall: 0.444444  | accracy:0.44444  | f1_score:0.45887  | auc:0.33333\n",
      "Eopch: 12\n",
      "Precision: 0.483333 recall: 0.444444  | accracy:0.44444  | f1_score:0.45887  | auc:0.33333\n",
      "Eopch: 13\n",
      "Precision: 0.483333 recall: 0.444444  | accracy:0.44444  | f1_score:0.45887  | auc:0.33333\n",
      "Eopch: 14\n",
      "Precision: 0.483333 recall: 0.444444  | accracy:0.44444  | f1_score:0.45887  | auc:0.33333\n",
      "Eopch: 15\n",
      "Precision: 0.483333 recall: 0.444444  | accracy:0.44444  | f1_score:0.45887  | auc:0.33333\n",
      "Eopch: 16\n",
      "Precision: 0.483333 recall: 0.444444  | accracy:0.44444  | f1_score:0.45887  | auc:0.33333\n",
      "Eopch: 17\n",
      "Precision: 0.483333 recall: 0.444444  | accracy:0.44444  | f1_score:0.45887  | auc:0.38889\n",
      "Eopch: 18\n",
      "Precision: 0.483333 recall: 0.444444  | accracy:0.44444  | f1_score:0.45887  | auc:0.38889\n",
      "Eopch: 19\n",
      "Precision: 0.483333 recall: 0.444444  | accracy:0.44444  | f1_score:0.45887  | auc:0.38889\n",
      "Eopch: 20\n",
      "Precision: 0.483333 recall: 0.444444  | accracy:0.44444  | f1_score:0.45887  | auc:0.38889\n",
      "Eopch: 21\n",
      "Precision: 0.555556 recall: 0.555556  | accracy:0.55556  | f1_score:0.55556  | auc:0.38889\n",
      "Eopch: 22\n",
      "Precision: 0.555556 recall: 0.555556  | accracy:0.55556  | f1_score:0.55556  | auc:0.38889\n",
      "Eopch: 23\n",
      "Precision: 0.642857 recall: 0.666667  | accracy:0.66667  | f1_score:0.64615  | auc:0.44444\n",
      "Eopch: 24\n",
      "Precision: 0.642857 recall: 0.666667  | accracy:0.66667  | f1_score:0.64615  | auc:0.44444\n",
      "Eopch: 25\n",
      "Precision: 0.642857 recall: 0.666667  | accracy:0.66667  | f1_score:0.64615  | auc:0.44444\n",
      "Eopch: 26\n",
      "Precision: 0.642857 recall: 0.666667  | accracy:0.66667  | f1_score:0.64615  | auc:0.44444\n",
      "Eopch: 27\n",
      "Precision: 0.642857 recall: 0.666667  | accracy:0.66667  | f1_score:0.64615  | auc:0.44444\n",
      "Eopch: 28\n",
      "Precision: 0.642857 recall: 0.666667  | accracy:0.66667  | f1_score:0.64615  | auc:0.50000\n",
      "Eopch: 29\n",
      "Precision: 0.642857 recall: 0.666667  | accracy:0.66667  | f1_score:0.64615  | auc:0.50000\n",
      "Eopch: 30\n",
      "Precision: 0.642857 recall: 0.666667  | accracy:0.66667  | f1_score:0.64615  | auc:0.50000\n",
      "Eopch: 31\n",
      "Precision: 0.642857 recall: 0.666667  | accracy:0.66667  | f1_score:0.64615  | auc:0.50000\n",
      "Eopch: 32\n",
      "Precision: 0.642857 recall: 0.666667  | accracy:0.66667  | f1_score:0.64615  | auc:0.50000\n",
      "Eopch: 33\n",
      "Precision: 0.642857 recall: 0.666667  | accracy:0.66667  | f1_score:0.64615  | auc:0.50000\n",
      "Eopch: 34\n",
      "Precision: 0.416667 recall: 0.555556  | accracy:0.55556  | f1_score:0.47619  | auc:0.50000\n",
      "Eopch: 35\n",
      "Precision: 0.416667 recall: 0.555556  | accracy:0.55556  | f1_score:0.47619  | auc:0.50000\n",
      "Eopch: 36\n",
      "Precision: 0.416667 recall: 0.555556  | accracy:0.55556  | f1_score:0.47619  | auc:0.50000\n",
      "Eopch: 37\n",
      "Precision: 0.444444 recall: 0.666667  | accracy:0.66667  | f1_score:0.53333  | auc:0.50000\n",
      "Eopch: 38\n",
      "Precision: 0.444444 recall: 0.666667  | accracy:0.66667  | f1_score:0.53333  | auc:0.50000\n",
      "Eopch: 39\n",
      "Precision: 0.444444 recall: 0.666667  | accracy:0.66667  | f1_score:0.53333  | auc:0.50000\n",
      "Eopch: 40\n",
      "Precision: 0.444444 recall: 0.666667  | accracy:0.66667  | f1_score:0.53333  | auc:0.50000\n",
      "Eopch: 41\n",
      "Precision: 0.444444 recall: 0.666667  | accracy:0.66667  | f1_score:0.53333  | auc:0.50000\n",
      "Eopch: 42\n",
      "Precision: 0.444444 recall: 0.666667  | accracy:0.66667  | f1_score:0.53333  | auc:0.50000\n",
      "Eopch: 43\n",
      "Precision: 0.444444 recall: 0.666667  | accracy:0.66667  | f1_score:0.53333  | auc:0.55556\n",
      "Eopch: 44\n",
      "Precision: 0.444444 recall: 0.666667  | accracy:0.66667  | f1_score:0.53333  | auc:0.55556\n",
      "Eopch: 45\n",
      "Precision: 0.444444 recall: 0.666667  | accracy:0.66667  | f1_score:0.53333  | auc:0.55556\n",
      "Eopch: 46\n",
      "Precision: 0.444444 recall: 0.666667  | accracy:0.66667  | f1_score:0.53333  | auc:0.55556\n",
      "Eopch: 47\n",
      "Precision: 0.444444 recall: 0.666667  | accracy:0.66667  | f1_score:0.53333  | auc:0.55556\n",
      "Eopch: 48\n",
      "Precision: 0.833333 recall: 0.777778  | accracy:0.77778  | f1_score:0.73810  | auc:0.55556\n",
      "Eopch: 49\n",
      "Precision: 0.833333 recall: 0.777778  | accracy:0.77778  | f1_score:0.73810  | auc:0.55556\n",
      "Eopch: 0\n",
      "Precision: 0.444444 recall: 0.666667  | accracy:0.66667  | f1_score:0.53333  | auc:0.94444\n",
      "Eopch: 1\n",
      "Precision: 0.916667 recall: 0.888889  | accracy:0.88889  | f1_score:0.89177  | auc:1.00000\n",
      "Eopch: 2\n",
      "Precision: 0.833333 recall: 0.666667  | accracy:0.66667  | f1_score:0.66667  | auc:1.00000\n",
      "Eopch: 3\n",
      "Precision: 0.444444 recall: 0.666667  | accracy:0.66667  | f1_score:0.53333  | auc:1.00000\n",
      "Eopch: 4\n",
      "Precision: 1.000000 recall: 1.000000  | accracy:1.00000  | f1_score:1.00000  | auc:1.00000\n",
      "Eopch: 5\n",
      "Precision: 0.833333 recall: 0.666667  | accracy:0.66667  | f1_score:0.66667  | auc:1.00000\n",
      "Eopch: 6\n",
      "Precision: 1.000000 recall: 1.000000  | accracy:1.00000  | f1_score:1.00000  | auc:1.00000\n",
      "Eopch: 7\n",
      "Precision: 0.444444 recall: 0.666667  | accracy:0.66667  | f1_score:0.53333  | auc:1.00000\n",
      "Eopch: 8\n",
      "Precision: 0.904762 recall: 0.888889  | accracy:0.88889  | f1_score:0.88205  | auc:1.00000\n",
      "Eopch: 9\n",
      "Precision: 1.000000 recall: 1.000000  | accracy:1.00000  | f1_score:1.00000  | auc:1.00000\n",
      "Eopch: 10\n",
      "Precision: 1.000000 recall: 1.000000  | accracy:1.00000  | f1_score:1.00000  | auc:1.00000\n",
      "Eopch: 11\n",
      "Precision: 1.000000 recall: 1.000000  | accracy:1.00000  | f1_score:1.00000  | auc:1.00000\n",
      "Eopch: 12\n",
      "Precision: 0.866667 recall: 0.777778  | accracy:0.77778  | f1_score:0.78333  | auc:1.00000\n",
      "Eopch: 13\n",
      "Precision: 0.866667 recall: 0.777778  | accracy:0.77778  | f1_score:0.78333  | auc:1.00000\n",
      "Eopch: 14\n",
      "Precision: 0.866667 recall: 0.777778  | accracy:0.77778  | f1_score:0.78333  | auc:1.00000\n",
      "Eopch: 15\n",
      "Precision: 0.866667 recall: 0.777778  | accracy:0.77778  | f1_score:0.78333  | auc:1.00000\n",
      "Eopch: 16\n",
      "Precision: 0.866667 recall: 0.777778  | accracy:0.77778  | f1_score:0.78333  | auc:1.00000\n",
      "Eopch: 17\n",
      "Precision: 0.833333 recall: 0.666667  | accracy:0.66667  | f1_score:0.66667  | auc:1.00000\n",
      "Eopch: 18\n",
      "Precision: 0.833333 recall: 0.666667  | accracy:0.66667  | f1_score:0.66667  | auc:1.00000\n",
      "Eopch: 19\n",
      "Precision: 0.833333 recall: 0.666667  | accracy:0.66667  | f1_score:0.66667  | auc:0.94444\n",
      "Eopch: 20\n",
      "Precision: 0.833333 recall: 0.666667  | accracy:0.66667  | f1_score:0.66667  | auc:0.94444\n",
      "Eopch: 21\n",
      "Precision: 0.833333 recall: 0.666667  | accracy:0.66667  | f1_score:0.66667  | auc:0.94444\n",
      "Eopch: 22\n",
      "Precision: 0.833333 recall: 0.666667  | accracy:0.66667  | f1_score:0.66667  | auc:0.94444\n",
      "Eopch: 23\n",
      "Precision: 0.833333 recall: 0.666667  | accracy:0.66667  | f1_score:0.66667  | auc:0.94444\n",
      "Eopch: 24\n",
      "Precision: 0.833333 recall: 0.666667  | accracy:0.66667  | f1_score:0.66667  | auc:0.94444\n",
      "Eopch: 25\n",
      "Precision: 0.833333 recall: 0.666667  | accracy:0.66667  | f1_score:0.66667  | auc:0.94444\n",
      "Eopch: 26\n",
      "Precision: 0.833333 recall: 0.666667  | accracy:0.66667  | f1_score:0.66667  | auc:0.94444\n",
      "Eopch: 27\n",
      "Precision: 0.833333 recall: 0.666667  | accracy:0.66667  | f1_score:0.66667  | auc:0.94444\n",
      "Eopch: 28\n",
      "Precision: 0.833333 recall: 0.666667  | accracy:0.66667  | f1_score:0.66667  | auc:0.94444\n",
      "Eopch: 29\n",
      "Precision: 0.833333 recall: 0.666667  | accracy:0.66667  | f1_score:0.66667  | auc:0.94444\n",
      "Eopch: 30\n",
      "Precision: 0.833333 recall: 0.666667  | accracy:0.66667  | f1_score:0.66667  | auc:0.94444\n",
      "Eopch: 31\n",
      "Precision: 0.833333 recall: 0.666667  | accracy:0.66667  | f1_score:0.66667  | auc:0.94444\n",
      "Eopch: 32\n",
      "Precision: 0.866667 recall: 0.777778  | accracy:0.77778  | f1_score:0.78333  | auc:0.94444\n",
      "Eopch: 33\n",
      "Precision: 0.866667 recall: 0.777778  | accracy:0.77778  | f1_score:0.78333  | auc:0.94444\n",
      "Eopch: 34\n",
      "Precision: 0.866667 recall: 0.777778  | accracy:0.77778  | f1_score:0.78333  | auc:0.94444\n",
      "Eopch: 35\n",
      "Precision: 0.916667 recall: 0.888889  | accracy:0.88889  | f1_score:0.89177  | auc:0.94444\n",
      "Eopch: 36\n",
      "Precision: 0.916667 recall: 0.888889  | accracy:0.88889  | f1_score:0.89177  | auc:0.94444\n",
      "Eopch: 37\n",
      "Precision: 0.916667 recall: 0.888889  | accracy:0.88889  | f1_score:0.89177  | auc:0.94444\n",
      "Eopch: 38\n",
      "Precision: 0.916667 recall: 0.888889  | accracy:0.88889  | f1_score:0.89177  | auc:0.94444\n",
      "Eopch: 39\n",
      "Precision: 0.916667 recall: 0.888889  | accracy:0.88889  | f1_score:0.89177  | auc:0.94444\n",
      "Eopch: 40\n",
      "Precision: 0.916667 recall: 0.888889  | accracy:0.88889  | f1_score:0.89177  | auc:0.94444\n",
      "Eopch: 41\n",
      "Precision: 0.916667 recall: 0.888889  | accracy:0.88889  | f1_score:0.89177  | auc:0.94444\n",
      "Eopch: 42\n",
      "Precision: 0.916667 recall: 0.888889  | accracy:0.88889  | f1_score:0.89177  | auc:0.94444\n",
      "Eopch: 43\n",
      "Precision: 0.916667 recall: 0.888889  | accracy:0.88889  | f1_score:0.89177  | auc:0.94444\n",
      "Eopch: 44\n",
      "Precision: 0.916667 recall: 0.888889  | accracy:0.88889  | f1_score:0.89177  | auc:0.94444\n",
      "Eopch: 45\n",
      "Precision: 0.916667 recall: 0.888889  | accracy:0.88889  | f1_score:0.89177  | auc:0.94444\n",
      "Eopch: 46\n",
      "Precision: 0.916667 recall: 0.888889  | accracy:0.88889  | f1_score:0.89177  | auc:0.94444\n",
      "Eopch: 47\n",
      "Precision: 0.916667 recall: 0.888889  | accracy:0.88889  | f1_score:0.89177  | auc:0.94444\n",
      "Eopch: 48\n",
      "Precision: 0.916667 recall: 0.888889  | accracy:0.88889  | f1_score:0.89177  | auc:0.94444\n",
      "Eopch: 49\n",
      "Precision: 0.916667 recall: 0.888889  | accracy:0.88889  | f1_score:0.89177  | auc:0.94444\n",
      "Eopch: 0\n",
      "Precision: 0.444444 recall: 0.666667  | accracy:0.66667  | f1_score:0.53333  | auc:0.16667\n",
      "Eopch: 1\n",
      "Precision: 0.444444 recall: 0.666667  | accracy:0.66667  | f1_score:0.53333  | auc:0.72222\n",
      "Eopch: 2\n",
      "Precision: 0.444444 recall: 0.666667  | accracy:0.66667  | f1_score:0.53333  | auc:0.72222\n",
      "Eopch: 3\n",
      "Precision: 0.444444 recall: 0.666667  | accracy:0.66667  | f1_score:0.53333  | auc:0.66667\n",
      "Eopch: 4\n",
      "Precision: 0.444444 recall: 0.666667  | accracy:0.66667  | f1_score:0.53333  | auc:0.66667\n",
      "Eopch: 5\n",
      "Precision: 0.444444 recall: 0.666667  | accracy:0.66667  | f1_score:0.53333  | auc:0.66667\n",
      "Eopch: 6\n",
      "Precision: 0.444444 recall: 0.666667  | accracy:0.66667  | f1_score:0.53333  | auc:0.66667\n",
      "Eopch: 7\n",
      "Precision: 0.444444 recall: 0.666667  | accracy:0.66667  | f1_score:0.53333  | auc:0.72222\n",
      "Eopch: 8\n",
      "Precision: 0.444444 recall: 0.666667  | accracy:0.66667  | f1_score:0.53333  | auc:0.72222\n",
      "Eopch: 9\n",
      "Precision: 0.444444 recall: 0.666667  | accracy:0.66667  | f1_score:0.53333  | auc:0.72222\n",
      "Eopch: 10\n",
      "Precision: 0.555556 recall: 0.555556  | accracy:0.55556  | f1_score:0.55556  | auc:0.72222\n",
      "Eopch: 11\n",
      "Precision: 0.555556 recall: 0.555556  | accracy:0.55556  | f1_score:0.55556  | auc:0.77778\n",
      "Eopch: 12\n",
      "Precision: 0.444444 recall: 0.666667  | accracy:0.66667  | f1_score:0.53333  | auc:0.77778\n",
      "Eopch: 13\n",
      "Precision: 0.642857 recall: 0.666667  | accracy:0.66667  | f1_score:0.64615  | auc:0.77778\n",
      "Eopch: 14\n",
      "Precision: 0.642857 recall: 0.666667  | accracy:0.66667  | f1_score:0.64615  | auc:0.77778\n",
      "Eopch: 15\n",
      "Precision: 0.642857 recall: 0.666667  | accracy:0.66667  | f1_score:0.64615  | auc:0.77778\n",
      "Eopch: 16\n",
      "Precision: 0.642857 recall: 0.666667  | accracy:0.66667  | f1_score:0.64615  | auc:0.77778\n",
      "Eopch: 17\n",
      "Precision: 0.833333 recall: 0.777778  | accracy:0.77778  | f1_score:0.73810  | auc:0.77778\n",
      "Eopch: 18\n",
      "Precision: 0.833333 recall: 0.777778  | accracy:0.77778  | f1_score:0.73810  | auc:0.77778\n",
      "Eopch: 19\n",
      "Precision: 0.642857 recall: 0.666667  | accracy:0.66667  | f1_score:0.64615  | auc:0.77778\n",
      "Eopch: 20\n",
      "Precision: 0.642857 recall: 0.666667  | accracy:0.66667  | f1_score:0.64615  | auc:0.77778\n",
      "Eopch: 21\n",
      "Precision: 0.642857 recall: 0.666667  | accracy:0.66667  | f1_score:0.64615  | auc:0.77778\n",
      "Eopch: 22\n",
      "Precision: 0.642857 recall: 0.666667  | accracy:0.66667  | f1_score:0.64615  | auc:0.77778\n",
      "Eopch: 23\n",
      "Precision: 0.833333 recall: 0.777778  | accracy:0.77778  | f1_score:0.73810  | auc:0.77778\n",
      "Eopch: 24\n",
      "Precision: 0.833333 recall: 0.777778  | accracy:0.77778  | f1_score:0.73810  | auc:0.77778\n",
      "Eopch: 25\n",
      "Precision: 0.833333 recall: 0.777778  | accracy:0.77778  | f1_score:0.73810  | auc:0.77778\n",
      "Eopch: 26\n",
      "Precision: 0.833333 recall: 0.777778  | accracy:0.77778  | f1_score:0.73810  | auc:0.77778\n",
      "Eopch: 27\n",
      "Precision: 0.833333 recall: 0.777778  | accracy:0.77778  | f1_score:0.73810  | auc:0.77778\n",
      "Eopch: 28\n",
      "Precision: 0.833333 recall: 0.777778  | accracy:0.77778  | f1_score:0.73810  | auc:0.77778\n",
      "Eopch: 29\n",
      "Precision: 0.833333 recall: 0.777778  | accracy:0.77778  | f1_score:0.73810  | auc:0.77778\n",
      "Eopch: 30\n",
      "Precision: 0.833333 recall: 0.777778  | accracy:0.77778  | f1_score:0.73810  | auc:0.66667\n",
      "Eopch: 31\n",
      "Precision: 0.833333 recall: 0.777778  | accracy:0.77778  | f1_score:0.73810  | auc:0.66667\n",
      "Eopch: 32\n",
      "Precision: 0.833333 recall: 0.777778  | accracy:0.77778  | f1_score:0.73810  | auc:0.66667\n",
      "Eopch: 33\n",
      "Precision: 0.833333 recall: 0.777778  | accracy:0.77778  | f1_score:0.73810  | auc:0.66667\n",
      "Eopch: 34\n",
      "Precision: 0.833333 recall: 0.777778  | accracy:0.77778  | f1_score:0.73810  | auc:0.83333\n",
      "Eopch: 35\n",
      "Precision: 0.642857 recall: 0.666667  | accracy:0.66667  | f1_score:0.64615  | auc:0.88889\n",
      "Eopch: 36\n",
      "Precision: 0.642857 recall: 0.666667  | accracy:0.66667  | f1_score:0.64615  | auc:0.88889\n",
      "Eopch: 37\n",
      "Precision: 0.642857 recall: 0.666667  | accracy:0.66667  | f1_score:0.64615  | auc:0.88889\n",
      "Eopch: 38\n",
      "Precision: 0.642857 recall: 0.666667  | accracy:0.66667  | f1_score:0.64615  | auc:0.88889\n",
      "Eopch: 39\n",
      "Precision: 0.642857 recall: 0.666667  | accracy:0.66667  | f1_score:0.64615  | auc:0.88889\n",
      "Eopch: 40\n",
      "Precision: 0.642857 recall: 0.666667  | accracy:0.66667  | f1_score:0.64615  | auc:0.88889\n",
      "Eopch: 41\n",
      "Precision: 0.642857 recall: 0.666667  | accracy:0.66667  | f1_score:0.64615  | auc:0.88889\n",
      "Eopch: 42\n",
      "Precision: 0.642857 recall: 0.666667  | accracy:0.66667  | f1_score:0.64615  | auc:0.88889\n",
      "Eopch: 43\n",
      "Precision: 0.642857 recall: 0.666667  | accracy:0.66667  | f1_score:0.64615  | auc:0.88889\n",
      "Eopch: 44\n",
      "Precision: 0.642857 recall: 0.666667  | accracy:0.66667  | f1_score:0.64615  | auc:0.88889\n",
      "Eopch: 45\n",
      "Precision: 0.642857 recall: 0.666667  | accracy:0.66667  | f1_score:0.64615  | auc:0.88889\n",
      "Eopch: 46\n",
      "Precision: 0.642857 recall: 0.666667  | accracy:0.66667  | f1_score:0.64615  | auc:0.88889\n",
      "Eopch: 47\n",
      "Precision: 0.642857 recall: 0.666667  | accracy:0.66667  | f1_score:0.64615  | auc:0.88889\n",
      "Eopch: 48\n",
      "Precision: 0.642857 recall: 0.666667  | accracy:0.66667  | f1_score:0.64615  | auc:0.88889\n",
      "Eopch: 49\n",
      "Precision: 0.642857 recall: 0.666667  | accracy:0.66667  | f1_score:0.64615  | auc:0.88889\n",
      "Eopch: 0\n",
      "Precision: 0.400000 recall: 0.333333  | accracy:0.33333  | f1_score:0.35000  | auc:0.33333\n",
      "Eopch: 1\n",
      "Precision: 0.416667 recall: 0.555556  | accracy:0.55556  | f1_score:0.47619  | auc:0.27778\n",
      "Eopch: 2\n",
      "Precision: 0.555556 recall: 0.555556  | accracy:0.55556  | f1_score:0.55556  | auc:0.27778\n",
      "Eopch: 3\n",
      "Precision: 0.047619 recall: 0.111111  | accracy:0.11111  | f1_score:0.06667  | auc:0.22222\n",
      "Eopch: 4\n",
      "Precision: 0.083333 recall: 0.222222  | accracy:0.22222  | f1_score:0.12121  | auc:0.22222\n",
      "Eopch: 5\n",
      "Precision: 0.400000 recall: 0.333333  | accracy:0.33333  | f1_score:0.35000  | auc:0.22222\n",
      "Eopch: 6\n",
      "Precision: 0.400000 recall: 0.333333  | accracy:0.33333  | f1_score:0.35000  | auc:0.22222\n",
      "Eopch: 7\n",
      "Precision: 0.400000 recall: 0.333333  | accracy:0.33333  | f1_score:0.35000  | auc:0.27778\n",
      "Eopch: 8\n",
      "Precision: 0.400000 recall: 0.333333  | accracy:0.33333  | f1_score:0.35000  | auc:0.27778\n",
      "Eopch: 9\n",
      "Precision: 0.400000 recall: 0.333333  | accracy:0.33333  | f1_score:0.35000  | auc:0.33333\n",
      "Eopch: 10\n",
      "Precision: 0.483333 recall: 0.444444  | accracy:0.44444  | f1_score:0.45887  | auc:0.38889\n",
      "Eopch: 11\n",
      "Precision: 0.483333 recall: 0.444444  | accracy:0.44444  | f1_score:0.45887  | auc:0.38889\n",
      "Eopch: 12\n",
      "Precision: 0.483333 recall: 0.444444  | accracy:0.44444  | f1_score:0.45887  | auc:0.38889\n",
      "Eopch: 13\n",
      "Precision: 0.483333 recall: 0.444444  | accracy:0.44444  | f1_score:0.45887  | auc:0.44444\n",
      "Eopch: 14\n",
      "Precision: 0.483333 recall: 0.444444  | accracy:0.44444  | f1_score:0.45887  | auc:0.50000\n",
      "Eopch: 15\n",
      "Precision: 0.483333 recall: 0.444444  | accracy:0.44444  | f1_score:0.45887  | auc:0.50000\n",
      "Eopch: 16\n",
      "Precision: 0.633333 recall: 0.555556  | accracy:0.55556  | f1_score:0.56667  | auc:0.50000\n",
      "Eopch: 17\n",
      "Precision: 0.633333 recall: 0.555556  | accracy:0.55556  | f1_score:0.56667  | auc:0.55556\n",
      "Eopch: 18\n",
      "Precision: 0.633333 recall: 0.555556  | accracy:0.55556  | f1_score:0.56667  | auc:0.61111\n",
      "Eopch: 19\n",
      "Precision: 0.633333 recall: 0.555556  | accracy:0.55556  | f1_score:0.56667  | auc:0.61111\n",
      "Eopch: 20\n",
      "Precision: 0.633333 recall: 0.555556  | accracy:0.55556  | f1_score:0.56667  | auc:0.61111\n",
      "Eopch: 21\n",
      "Precision: 0.633333 recall: 0.555556  | accracy:0.55556  | f1_score:0.56667  | auc:0.61111\n",
      "Eopch: 22\n",
      "Precision: 0.833333 recall: 0.666667  | accracy:0.66667  | f1_score:0.66667  | auc:0.61111\n",
      "Eopch: 23\n",
      "Precision: 0.833333 recall: 0.666667  | accracy:0.66667  | f1_score:0.66667  | auc:0.61111\n",
      "Eopch: 24\n",
      "Precision: 0.833333 recall: 0.666667  | accracy:0.66667  | f1_score:0.66667  | auc:0.61111\n",
      "Eopch: 25\n",
      "Precision: 0.700000 recall: 0.666667  | accracy:0.66667  | f1_score:0.67532  | auc:0.61111\n",
      "Eopch: 26\n",
      "Precision: 0.633333 recall: 0.555556  | accracy:0.55556  | f1_score:0.56667  | auc:0.61111\n",
      "Eopch: 27\n",
      "Precision: 0.700000 recall: 0.666667  | accracy:0.66667  | f1_score:0.67532  | auc:0.61111\n",
      "Eopch: 28\n",
      "Precision: 0.700000 recall: 0.666667  | accracy:0.66667  | f1_score:0.67532  | auc:0.66667\n",
      "Eopch: 29\n",
      "Precision: 0.700000 recall: 0.666667  | accracy:0.66667  | f1_score:0.67532  | auc:0.66667\n",
      "Eopch: 30\n",
      "Precision: 0.700000 recall: 0.666667  | accracy:0.66667  | f1_score:0.67532  | auc:0.72222\n",
      "Eopch: 31\n",
      "Precision: 0.700000 recall: 0.666667  | accracy:0.66667  | f1_score:0.67532  | auc:0.77778\n",
      "Eopch: 32\n",
      "Precision: 0.700000 recall: 0.666667  | accracy:0.66667  | f1_score:0.67532  | auc:0.77778\n",
      "Eopch: 33\n",
      "Precision: 0.700000 recall: 0.666667  | accracy:0.66667  | f1_score:0.67532  | auc:0.77778\n",
      "Eopch: 34\n",
      "Precision: 0.700000 recall: 0.666667  | accracy:0.66667  | f1_score:0.67532  | auc:0.77778\n",
      "Eopch: 35\n",
      "Precision: 0.700000 recall: 0.666667  | accracy:0.66667  | f1_score:0.67532  | auc:0.77778\n",
      "Eopch: 36\n",
      "Precision: 0.700000 recall: 0.666667  | accracy:0.66667  | f1_score:0.67532  | auc:0.77778\n",
      "Eopch: 37\n",
      "Precision: 0.700000 recall: 0.666667  | accracy:0.66667  | f1_score:0.67532  | auc:0.77778\n",
      "Eopch: 38\n",
      "Precision: 0.700000 recall: 0.666667  | accracy:0.66667  | f1_score:0.67532  | auc:0.77778\n",
      "Eopch: 39\n",
      "Precision: 0.700000 recall: 0.666667  | accracy:0.66667  | f1_score:0.67532  | auc:0.77778\n",
      "Eopch: 40\n",
      "Precision: 0.700000 recall: 0.666667  | accracy:0.66667  | f1_score:0.67532  | auc:0.77778\n",
      "Eopch: 41\n",
      "Precision: 0.700000 recall: 0.666667  | accracy:0.66667  | f1_score:0.67532  | auc:0.77778\n",
      "Eopch: 42\n",
      "Precision: 0.700000 recall: 0.666667  | accracy:0.66667  | f1_score:0.67532  | auc:0.72222\n",
      "Eopch: 43\n",
      "Precision: 0.700000 recall: 0.666667  | accracy:0.66667  | f1_score:0.67532  | auc:0.66667\n",
      "Eopch: 44\n",
      "Precision: 0.700000 recall: 0.666667  | accracy:0.66667  | f1_score:0.67532  | auc:0.61111\n",
      "Eopch: 45\n",
      "Precision: 0.700000 recall: 0.666667  | accracy:0.66667  | f1_score:0.67532  | auc:0.61111\n",
      "Eopch: 46\n",
      "Precision: 0.700000 recall: 0.666667  | accracy:0.66667  | f1_score:0.67532  | auc:0.66667\n",
      "Eopch: 47\n",
      "Precision: 0.700000 recall: 0.666667  | accracy:0.66667  | f1_score:0.67532  | auc:0.66667\n",
      "Eopch: 48\n",
      "Precision: 0.700000 recall: 0.666667  | accracy:0.66667  | f1_score:0.67532  | auc:0.66667\n",
      "Eopch: 49\n",
      "Precision: 0.700000 recall: 0.666667  | accracy:0.66667  | f1_score:0.67532  | auc:0.66667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.optim import Adam,SGD\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "import copy\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from model.gdotnet import CRATE_edgev3\n",
    "from sklearn.model_selection import train_test_split\n",
    "device = 'cuda'\n",
    "best_state = 0\n",
    "best_state_acc = 0\n",
    "best_f1_score = 0\n",
    "best_auc = 0\n",
    "fold = 5\n",
    "res_acc = []\n",
    "res_f1_score = []\n",
    "res_auc = []\n",
    "res_pre = []\n",
    "res_rec = []\n",
    "result_acc = 0\n",
    "result_auc = 0\n",
    "result_f1_score = 0\n",
    "result_recall = 0\n",
    "result_pre = 0\n",
    "\n",
    "torch.manual_seed(49)\n",
    "# kf = StratifiedKFold(n_splits=fold,shuffle=True,random_state=70) #70\n",
    "    # kf = KFold(n_splits=fold,shuffle=True,random_state=0)\n",
    "fold_no = 1\n",
    "result_acc = 0\n",
    "result_auc = 0\n",
    "result_f1_score = 0\n",
    "result_recall = 0\n",
    "result_pre = 0\n",
    "iters = 5\n",
    "for i in range(iters):\n",
    "    kf = StratifiedKFold(n_splits=fold,shuffle=True, random_state=70) #70\n",
    "    for train_index, val_index in kf.split(all_data_graph, all_label):\n",
    "        # train_subset, train_graph, train_label = all_data[train_index], all_data_graph[train_index], all_label[train_index]\n",
    "        # val_subset, val_graph, val_label = all_data[val_index], all_data_graph[val_index], all_label[val_index]\n",
    "        train_ratio = 0.7\n",
    "        val_ratio = 0.1\n",
    "        test_ratio = 0.2\n",
    "        indices = np.arange(len(all_label))\n",
    "        train_index, temp_index, train_label_split, temp_label_split = train_test_split(\n",
    "            indices, \n",
    "            all_label,\n",
    "            test_size=(val_ratio + test_ratio), # 0.1 + 0.2 = 0.3\n",
    "            shuffle=True,\n",
    "            stratify=all_label,# for reproducible results\n",
    "        )\n",
    "\n",
    "        # Split 2: Split the 30% \"Temp\" set into Validation (10%) and Test (20%)\n",
    "        # The new test_size is relative to the temp set: \n",
    "        # test_ratio / (val_ratio + test_ratio) = 0.2 / 0.3 = 0.666...\n",
    "        val_index, test_index = train_test_split(\n",
    "            temp_index,\n",
    "            test_size=(test_ratio / (val_ratio + test_ratio)),\n",
    "            shuffle=True,\n",
    "            stratify=temp_label_split, # Stratify based on the temp set's labels\n",
    "            random_state=42 # for reproducible results\n",
    "        )\n",
    "\n",
    "\n",
    "        # Training set (70%)\n",
    "        train_subset, train_graph, train_label = all_data[train_index], all_data_graph[train_index], all_label[train_index]\n",
    "\n",
    "        # Validation set (10%)\n",
    "        val_subset, val_graph, val_label = all_data[val_index], all_data_graph[val_index], all_label[val_index]\n",
    "\n",
    "        # Test set (20%)\n",
    "        test_subset, test_graph, test_label = all_data[test_index], all_data_graph[test_index], all_label[test_index]\n",
    "        train_label = train_label.squeeze(1)\n",
    "        val_label = val_label.squeeze(1)\n",
    "        test_label = test_label.squeeze(1)\n",
    "        train_label = F.one_hot(train_label.to(torch.int64))\n",
    "        val_label = F.one_hot(val_label.to(torch.int64))\n",
    "        test_label = F.one_hot(test_label.to(torch.int64))\n",
    "            # 创建DataLoader\n",
    "\n",
    "        train_set = torch.utils.data.TensorDataset(train_graph, train_subset,  train_label)\n",
    "        test_set = torch.utils.data.TensorDataset(test_graph, test_subset, test_label)\n",
    "        # val_set = torch.utils.data.TensorDataset(val_graph, val_subset, val_label)\n",
    "        train_loader = DataLoader(train_set, batch_size=512, num_workers=0, shuffle=True)\n",
    "        val_loader = DataLoader(test_set, batch_size=512, num_workers=0, shuffle=False)\n",
    "        # test_loader = DataLoader(test_set, batch_size=512, num_workers=0, shuffle=True)\n",
    "\n",
    "        loss_func = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "        lr = 0.0001\n",
    "        lr_scheduler= LRScheduler()\n",
    "        current_step = 0\n",
    "\n",
    "        epochs = 50\n",
    "        flag_epoch = 0\n",
    "        best_accuray = 0\n",
    "        best_model = 0\n",
    "        best_pre = 0\n",
    "        best_rec = 0\n",
    "        g_model = CRATE_edgev3()\n",
    "        # g_model = subgraphkan()\n",
    "        g_model = g_model.to(device)\n",
    "\n",
    "        g_optimizer = Adam(g_model.parameters(), lr=lr, weight_decay=0.02)\n",
    "        for epoch in range(epochs):\n",
    "            prob_all = []\n",
    "            label_all = []\n",
    "            auc_all = []\n",
    "            total_loss = 0\n",
    "            accuracy = 0\n",
    "            g_model.train()\n",
    "            for batch_idx, (train_graph, train_data, training_labels) in enumerate(train_loader):\n",
    "                current_step += 1\n",
    "                lr_scheduler.update(optimizer=g_optimizer, step=current_step)\n",
    "                train_data, train_graph, training_labels = train_data.to(device), train_graph.to(device), training_labels.to(device)\n",
    "                training_labels = training_labels.float()\n",
    "                g_optimizer.zero_grad()\n",
    "                pred_label ,output= g_model(train_graph, train_data, training_labels[:,1].long())\n",
    "                \n",
    "                loss = loss_func(pred_label, training_labels)\n",
    "                loss.backward()\n",
    "                g_optimizer.step() \n",
    "                training_labels = torch.max(training_labels, 1)[1].squeeze()\n",
    "                pred_y = torch.max(pred_label,1)[1].squeeze() #squeeze()默认是将a中所有为1的维度删掉\n",
    "                            #pred_size() = [10000]\n",
    "                accuracy += sum(pred_y == training_labels) / train_data.__len__()\n",
    "            # print('Eopch:', int(epoch), ' | train loss: %.6f' % loss.item(), ' | accracy:%.5f' % (accuracy/(batch_idx+1)))\n",
    "            g_model.eval()\n",
    "            accuracy = 0\n",
    "            result = []\n",
    "            labels = []\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (train_graph, train_data, training_labels) in enumerate(val_loader):\n",
    "                    train_data, training_labels = train_data.to(device), training_labels.to(device)\n",
    "                    train_graph = train_graph.to(device)\n",
    "                    pred_label,_ = g_model(train_graph, train_data, None)\n",
    "                    \n",
    "                    # training_labels = training_labels.float()\n",
    "                    result += F.softmax(pred_label, dim=1)[:, 1].tolist()\n",
    "                    labels += training_labels[:, 1].tolist()\n",
    "                    pred_y = torch.max(pred_label,1)[1].squeeze() #squeeze()默认是将a中所有为1的维度删掉\n",
    "                                #pred_size() = [10000]\n",
    "                    pred_label = F.softmax(pred_label,dim=1)\n",
    "                    training_labels = training_labels[:,1]\n",
    "                    accuracy += sum(pred_y == training_labels) / train_data.__len__()\n",
    "                    prob_all.extend(pred_y.cpu().numpy())\n",
    "                    label_all.extend(training_labels.cpu().numpy())\n",
    "                    auc_all.extend(pred_label[:, 1].detach().cpu().numpy())\n",
    "            auc = roc_auc_score(labels, result,average='weighted')\n",
    "            result, labels = np.array(result), np.array(labels)\n",
    "            result[result >= 0.5] = 1\n",
    "            result[result < 0.5] = 0\n",
    "            f1 = f1_score(labels, result, average='weighted')\n",
    "            rec = recall_score(labels, result, average='weighted')\n",
    "            pre = precision_score(labels, result, average='weighted')\n",
    "            accuracy = accuracy/(batch_idx+1)\n",
    "            if accuracy>best_accuray:\n",
    "                # print(accuracy,pre,rec)\n",
    "                best_accuray = accuracy \n",
    "                    # best_model = copy.deepcopy(g_model)\n",
    "                best_auc = auc\n",
    "                best_f1_score = f1\n",
    "                best_pre = pre\n",
    "                best_rec = rec\n",
    "        # break\n",
    "\n",
    "            print('Eopch:', int(epoch))\n",
    "            print('Precision: %.6f'%(pre), 'recall: %.6f'%(rec),' | accracy:%.5f' % (accuracy), ' | f1_score:%.5f' % (f1), ' | auc:%.5f' % (auc))\n",
    "            # print(\"****************************************\")\n",
    "        break\n",
    "    \n",
    "\n",
    "    res_acc.append(best_accuray.cpu().numpy())\n",
    "    res_auc.append(best_auc)\n",
    "    res_f1_score.append(best_f1_score)\n",
    "    res_pre.append(best_pre)\n",
    "    res_rec.append(best_rec)\n",
    "        # print('best_Precision: %.6f'%(best_pre), 'best_recall: %.6f'%(best_rec),' | accracy:%.5f' % (best_accuray), ' | f1_score:%.5f' % (best_f1_score), ' | auc:%.5f' % (best_auc))\n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0018bb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9d7f1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.8\n",
      "f1: 0.7841269841269841\n",
      "auc: 0.7111111111111111\n",
      "pre: 0.8555555555555555\n",
      "rec: 0.7999999999999999\n"
     ]
    }
   ],
   "source": [
    "acc_f = np.mean(res_acc)\n",
    "f1_f = np.mean(res_f1_score)\n",
    "auc_f = np.mean(res_auc)\n",
    "pre_f = np.mean(res_pre)\n",
    "rec_f = np.mean(res_rec)\n",
    "print(\"acc:\", acc_f)\n",
    "print(\"f1:\", f1_f)\n",
    "print(\"auc:\", auc_f)\n",
    "print(\"pre:\", pre_f)\n",
    "print(\"rec:\", rec_f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hgt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
